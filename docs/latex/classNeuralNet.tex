\hypertarget{classNeuralNet}{
\section{NeuralNet Class Reference}
\label{classNeuralNet}\index{NeuralNet@{NeuralNet}}
}
{\tt \#include $<$NeuralNet.h$>$}

\subsection*{Public Member Functions}
\begin{CompactItemize}
\item 
\hyperlink{classNeuralNet_a7045d229a664d8209a919d93108426a}{NeuralNet} (int \hyperlink{classNeuralNet_d46f938b28b505ef25f9d0a3a256ba78}{numInput}, int \hyperlink{classNeuralNet_62cfa0d0238baf0239429fbefd63042c}{numHidden}, int \hyperlink{classNeuralNet_c20e9fd588f7be05e8d658a5b673affe}{numOutput})
\item 
void \hyperlink{classNeuralNet_ae2e3914f799ae37da4f3d088e5259db}{train} ()
\item 
void \hyperlink{classNeuralNet_3ae443739e4e6a6c0d86d5149bde706f}{compute} ()
\item 
void \hyperlink{classNeuralNet_f36ed795560a187aaafc86b82348ae05}{calculateNeuronValues} (\hyperlink{classGenericLayer}{GenericLayer} $\ast$layer)
\item 
double \hyperlink{classNeuralNet_8a49e8b1bf5710e27c57b322ee3bc323}{logisticActivation} (double x)
\item 
void \hyperlink{classNeuralNet_ec1f7f481954c7a3eafdefc8d23070f8}{alterWeights} (\hyperlink{classGenericLayer}{GenericLayer} $\ast$layer)
\item 
double \hyperlink{classNeuralNet_ae5a77ddecea2c9f0c11feb0089dbb03}{calculateMSE} ()
\end{CompactItemize}
\subsection*{Public Attributes}
\begin{CompactItemize}
\item 
double \hyperlink{classNeuralNet_dbc3025a07c81b26fddcc6f376f69cd1}{learningRate} = 0.25
\item 
int \hyperlink{classNeuralNet_48dd706c84da839dca865eae8cdffc5e}{maxTrainingIterations} = 1000
\item 
double \hyperlink{classNeuralNet_a8329b04a9c709b1b508b06dbec234bd}{percentChange} = 0.01
\item 
\hyperlink{classGenericLayer}{GenericLayer} $\ast$ \hyperlink{classNeuralNet_a254ed58d525fb163fe78e8ad2013d5e}{input}
\item 
\hyperlink{classGenericLayer}{GenericLayer} $\ast$ \hyperlink{classNeuralNet_b0c566728137bb3bb4c3328005dcb33b}{hidden}
\item 
\hyperlink{classGenericLayer}{GenericLayer} $\ast$ \hyperlink{classNeuralNet_97e58ab98eff78a898e9bb68ebc5be11}{output}
\item 
int \hyperlink{classNeuralNet_d46f938b28b505ef25f9d0a3a256ba78}{numInput}
\item 
int \hyperlink{classNeuralNet_62cfa0d0238baf0239429fbefd63042c}{numHidden}
\item 
int \hyperlink{classNeuralNet_c20e9fd588f7be05e8d658a5b673affe}{numOutput}
\item 
double $\ast$ \hyperlink{classNeuralNet_db3310c1abfe25972101f3a5f9b45d1e}{desiredOutput}
\item 
double $\ast$ \hyperlink{classNeuralNet_a767b858e21d79356577a77f927080fa}{inputData}
\end{CompactItemize}


\subsection{Detailed Description}
This class brings together the work done by various parts of the Neural system. It is the binding layer between the neural layers and the data structures inherent in the system. As such, it is responsible for initializing, stucturing, training, and calculating the neural network environment. 

\subsection{Constructor \& Destructor Documentation}
\hypertarget{classNeuralNet_a7045d229a664d8209a919d93108426a}{
\index{NeuralNet@{NeuralNet}!NeuralNet@{NeuralNet}}
\index{NeuralNet@{NeuralNet}!NeuralNet@{NeuralNet}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}NeuralNet::NeuralNet (int {\em numInput}, int {\em numHidden}, int {\em numOutput})}}
\label{classNeuralNet_a7045d229a664d8209a919d93108426a}


\begin{Desc}
\item[Parameters:]
\begin{description}
\item[{\em numInput}]The number of neurons to create in the input layer. \item[{\em numHidden}]The number of neurons to create in the hidden layer. \item[{\em numOutput}]The number of neurons to create in the output layer. \end{description}
\end{Desc}


\subsection{Member Function Documentation}
\hypertarget{classNeuralNet_ae2e3914f799ae37da4f3d088e5259db}{
\index{NeuralNet@{NeuralNet}!train@{train}}
\index{train@{train}!NeuralNet@{NeuralNet}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}void NeuralNet::train ()}}
\label{classNeuralNet_ae2e3914f799ae37da4f3d088e5259db}


\begin{Desc}
\item[Precondition:]The trainingData and desiredValues arrays have been assigned and are of the appropriate dimensions for this particular NN. \end{Desc}
\begin{Desc}
\item[Postcondition:]All the weights and biases are set according to the trainingData and the NN is ready for real environment data. \end{Desc}
\hypertarget{classNeuralNet_3ae443739e4e6a6c0d86d5149bde706f}{
\index{NeuralNet@{NeuralNet}!compute@{compute}}
\index{compute@{compute}!NeuralNet@{NeuralNet}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}void NeuralNet::compute ()}}
\label{classNeuralNet_3ae443739e4e6a6c0d86d5149bde706f}


\begin{Desc}
\item[Precondition:]The NN has been initialized and trained, and the inputData array has been assigned and is of the appropriate dimensions for this particular NN. \end{Desc}
\begin{Desc}
\item[Postcondition:]The output layer nodes now contain, based on the NN's weights and biases, the output values for the given data set. \end{Desc}
\hypertarget{classNeuralNet_f36ed795560a187aaafc86b82348ae05}{
\index{NeuralNet@{NeuralNet}!calculateNeuronValues@{calculateNeuronValues}}
\index{calculateNeuronValues@{calculateNeuronValues}!NeuralNet@{NeuralNet}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}void NeuralNet::calculateNeuronValues ({\bf GenericLayer} $\ast$ {\em layer})}}
\label{classNeuralNet_f36ed795560a187aaafc86b82348ae05}


To calculate the value of each neuron we calculate the sum of the weights connected to each neuron multiplied by the value of each corresponding neuronal value, finally adding the adjusted bias and passing this value through an activation function.

The formula we use is given as follows, where $y_i$ is the i-th neuron on the parent layer and $w_{ji}$ is the weight from the parent to the neuron whose value we are calculating (note that in this notation, for simplicity, we denote the bias and its weight as the first element): $\displaystyle v_j(n) = \sum_{i=0}^n y_i(n)w_{ji}(n)$

And similarly we note that the final value of this neuron is $y(n)_j = \phi_j(v_j(n))$ where $\phi_j()$ is the activation function on neuron j. \hypertarget{classNeuralNet_8a49e8b1bf5710e27c57b322ee3bc323}{
\index{NeuralNet@{NeuralNet}!logisticActivation@{logisticActivation}}
\index{logisticActivation@{logisticActivation}!NeuralNet@{NeuralNet}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}double NeuralNet::logisticActivation (double {\em x})}}
\label{classNeuralNet_8a49e8b1bf5710e27c57b322ee3bc323}


Used for computing the value of a neuron after calculating the raw value. We use an easily differentiable function so that it is easier later on to calculate the change in weights. The particular function we use here is the sigmoidal activation function, which constrains the output to between 0 and +1, a good fit for NNs.

The precise function we use is $\displaystyle\phi_j(x) = \frac{1}{1 + exp(-x)}$. \hypertarget{classNeuralNet_ec1f7f481954c7a3eafdefc8d23070f8}{
\index{NeuralNet@{NeuralNet}!alterWeights@{alterWeights}}
\index{alterWeights@{alterWeights}!NeuralNet@{NeuralNet}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}void NeuralNet::alterWeights ({\bf GenericLayer} $\ast$ {\em layer})}}
\label{classNeuralNet_ec1f7f481954c7a3eafdefc8d23070f8}


We calculate the errors of the each of the neurons in the NN here for use in adjusting their weights and biases for training purposes. There are three main cases for a three layer feed forward NN.

The first is the input layer, and since the desired value is always equal to the actual value, since it is given, the gradient is 0.

The second is the case of a hidden layer, where the errors are calculated from the layer nearest to the output layer to the layer closest to the input layer. This is done using the back propogation algorithm, which, given the gradients of adjacent layers, calculates the gradients recursively working backwords and using the derivative of the activation function used for calculating neuron values. This is done since the error signals cannot be determined for hidden layers since there is no value to compare their output to. It can be written as $\displaystyle\gamma_j(n) = \phi_j^`(v_j(n))\sum_{k=0}^m \gamma_k(n)w_{kj}(n)$ where neuron j is the gradient we are calculating, neuron k is in the child layer and m is the number of neurons on that layer.

The third case is the output layer, where the error is trivially desired-actual. The gradient is then defined much the same as case 2 where it is the error multiplied by the derivative of the activation function applied to the value of the neuron. It can be written as $\displaystyle\gamma_j(n) = e_j(n)\phi_j^`(v_j(n))$.

\begin{Desc}
\item[Parameters:]
\begin{description}
\item[{\em layer}]The layer for which we are to calculate the errors. \end{description}
\end{Desc}
\hypertarget{classNeuralNet_ae5a77ddecea2c9f0c11feb0089dbb03}{
\index{NeuralNet@{NeuralNet}!calculateMSE@{calculateMSE}}
\index{calculateMSE@{calculateMSE}!NeuralNet@{NeuralNet}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}double NeuralNet::calculateMSE ()}}
\label{classNeuralNet_ae5a77ddecea2c9f0c11feb0089dbb03}


\begin{Desc}
\item[Returns:]The mean squared error of the output layer. \end{Desc}


\subsection{Member Data Documentation}
\hypertarget{classNeuralNet_dbc3025a07c81b26fddcc6f376f69cd1}{
\index{NeuralNet@{NeuralNet}!learningRate@{learningRate}}
\index{learningRate@{learningRate}!NeuralNet@{NeuralNet}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}double {\bf NeuralNet::learningRate} = 0.25}}
\label{classNeuralNet_dbc3025a07c81b26fddcc6f376f69cd1}


This variable controls the rate at which the network learns. It is responsible for smoothing out the learning functions. \hypertarget{classNeuralNet_48dd706c84da839dca865eae8cdffc5e}{
\index{NeuralNet@{NeuralNet}!maxTrainingIterations@{maxTrainingIterations}}
\index{maxTrainingIterations@{maxTrainingIterations}!NeuralNet@{NeuralNet}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}int {\bf NeuralNet::maxTrainingIterations} = 1000}}
\label{classNeuralNet_48dd706c84da839dca865eae8cdffc5e}


The max number of iterations to compute while training. \hypertarget{classNeuralNet_a8329b04a9c709b1b508b06dbec234bd}{
\index{NeuralNet@{NeuralNet}!percentChange@{percentChange}}
\index{percentChange@{percentChange}!NeuralNet@{NeuralNet}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}double {\bf NeuralNet::percentChange} = 0.01}}
\label{classNeuralNet_a8329b04a9c709b1b508b06dbec234bd}


When to stop the training based on each epoch's mean squared error. A percentage of the rate of change. \hypertarget{classNeuralNet_a254ed58d525fb163fe78e8ad2013d5e}{
\index{NeuralNet@{NeuralNet}!input@{input}}
\index{input@{input}!NeuralNet@{NeuralNet}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}{\bf GenericLayer}$\ast$ {\bf NeuralNet::input}}}
\label{classNeuralNet_a254ed58d525fb163fe78e8ad2013d5e}


The input layer to the neural network. \hypertarget{classNeuralNet_b0c566728137bb3bb4c3328005dcb33b}{
\index{NeuralNet@{NeuralNet}!hidden@{hidden}}
\index{hidden@{hidden}!NeuralNet@{NeuralNet}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}{\bf GenericLayer}$\ast$ {\bf NeuralNet::hidden}}}
\label{classNeuralNet_b0c566728137bb3bb4c3328005dcb33b}


The hidden layer to the neural network. \hypertarget{classNeuralNet_97e58ab98eff78a898e9bb68ebc5be11}{
\index{NeuralNet@{NeuralNet}!output@{output}}
\index{output@{output}!NeuralNet@{NeuralNet}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}{\bf GenericLayer}$\ast$ {\bf NeuralNet::output}}}
\label{classNeuralNet_97e58ab98eff78a898e9bb68ebc5be11}


The output layer to the neural network. \hypertarget{classNeuralNet_d46f938b28b505ef25f9d0a3a256ba78}{
\index{NeuralNet@{NeuralNet}!numInput@{numInput}}
\index{numInput@{numInput}!NeuralNet@{NeuralNet}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}int {\bf NeuralNet::numInput}}}
\label{classNeuralNet_d46f938b28b505ef25f9d0a3a256ba78}


The number of input neurons to create. \hypertarget{classNeuralNet_62cfa0d0238baf0239429fbefd63042c}{
\index{NeuralNet@{NeuralNet}!numHidden@{numHidden}}
\index{numHidden@{numHidden}!NeuralNet@{NeuralNet}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}int {\bf NeuralNet::numHidden}}}
\label{classNeuralNet_62cfa0d0238baf0239429fbefd63042c}


The number of hidden neurons to create. \hypertarget{classNeuralNet_c20e9fd588f7be05e8d658a5b673affe}{
\index{NeuralNet@{NeuralNet}!numOutput@{numOutput}}
\index{numOutput@{numOutput}!NeuralNet@{NeuralNet}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}int {\bf NeuralNet::numOutput}}}
\label{classNeuralNet_c20e9fd588f7be05e8d658a5b673affe}


The number of output neurons to create. \hypertarget{classNeuralNet_db3310c1abfe25972101f3a5f9b45d1e}{
\index{NeuralNet@{NeuralNet}!desiredOutput@{desiredOutput}}
\index{desiredOutput@{desiredOutput}!NeuralNet@{NeuralNet}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}double$\ast$ {\bf NeuralNet::desiredOutput}}}
\label{classNeuralNet_db3310c1abfe25972101f3a5f9b45d1e}


The expected results from the training data. Each element is related to each output neuron's expected value \hypertarget{classNeuralNet_a767b858e21d79356577a77f927080fa}{
\index{NeuralNet@{NeuralNet}!inputData@{inputData}}
\index{inputData@{inputData}!NeuralNet@{NeuralNet}}
\subsubsection{\setlength{\rightskip}{0pt plus 5cm}double$\ast$ {\bf NeuralNet::inputData}}}
\label{classNeuralNet_a767b858e21d79356577a77f927080fa}


The data to be calculated from the environment, in the same form as each set of data in trainingData. 

The documentation for this class was generated from the following files:\begin{CompactItemize}
\item 
src/\hyperlink{NeuralNet_8h}{NeuralNet.h}\item 
src/\hyperlink{NeuralNet_8cpp}{NeuralNet.cpp}\end{CompactItemize}
